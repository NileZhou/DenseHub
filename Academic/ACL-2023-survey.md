Overview

几大类别:

- Long Papers **912 papers** 经过完整研究周期的成果，包含完备的实验数据和分析
- Short Papers: **165 papers** 更精炼的，页数更少，但是很有意义的研究问题
- System Demonstration: **59 papers** 强调系统的设计、实现以及实际应用的效果。它们通常附有演示软件，允许会议参与者直接体验这些工具或系统
- Student Research Workshop: **35 papers** 旨在为学生提供交换思想和经验的平台
- Industry Track: **77 papers** 企业在自然语言处理领域的实际案例和成果展示
- Tutorial Abstracts: **7 papers** 教程摘要提供对某一特定话题或技术的深入教学和讨论

举行时间：2023.7.9 -- 2023.7.14

强调：Reality Check

特点：首次设立Industry Track

submission情况：4864 篇（较2022增加 1486 篇），其中301 篇是通过滚动审查（ARR）提交的

accept rate: Long / Short Papers = 22.13%/15.54%

# Timeline

PS: 这里的时间都指 11:59PM UTC-12:00

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=NzNiYzZjODJlN2ZhZTUyNTZkODU1MGYwMWQ5NDBlNmFfU0ZoU2FuaDNxa0pzbVQ5T3BHUTZFTmVFd0hVdW82OW5fVG9rZW46QnQ2emJhdW40bzI2dVp4TUk1Q2N6cWhWbkVoXzE3MDUzMzQzNDA6MTcwNTMzNzk0MF9WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=NzNiYzZjODJlN2ZhZTUyNTZkODU1MGYwMWQ5NDBlNmFfU0ZoU2FuaDNxa0pzbVQ5T3BHUTZFTmVFd0hVdW82OW5fVG9rZW46QnQ2emJhdW40bzI2dVp4TUk1Q2N6cWhWbkVoXzE3MDUzMzQzNDA6MTcwNTMzNzk0MF9WNA)

| 英文                                                        | 中文                               | 日期                 |
| ----------------------------------------------------------- | ---------------------------------- | -------------------- |
| Submission template available                               | 投稿模板可用                       | 2022年11月1日        |
| Anonymity period for ARR papers                             | ARR论文匿名期                      | 2022年11月15日       |
| Submission deadline for ARR                                 | ARR论文投稿截止日期                | 2022年12月15日       |
| Anonymity period for Softconf START submissions             | 通过Softconf START的论文匿名期     | 2022年12月20日       |
| Abstract deadline for Softconf START direct submissions     | Softconf START直投稿件摘要截止日期 | 2023年1月13日        |
| Direct paper submission deadline                            | 直接论文投稿截止日期               | 2023年1月20日        |
| Anonymity period for ARR papers to be committed to ACL 2023 | 承诺至ACL 2023的ARR论文匿名期      | 2023年2月17日        |
| Commitment deadline for ARR papers                          | ARR论文承诺截止日期                | 2023年3月17日        |
| Author response period                                      | 作者回应期                         | 2023年3月17日-24日   |
| Discussion period                                           | 讨论期                             | 2023年3月25日-4月7日 |
| Notification of acceptance                                  | 录取通知                           | 2023年5月1日         |
| Withdrawal deadline                                         | 撤回截止日期                       | 2023年5月8日         |
| Camera-ready papers due                                     | 定稿论文截止日期                   | 2023年5月25日        |
| Tutorials                                                   | 教程                               | 2023年7月9日         |
| Conference                                                  | 会议                               | 2023年7月10日-12日   |
| Workshops                                                   | 宣讲                               | 2023年7月13日-14日   |

# Submission Statistics

## Topic

官方推荐的，但不限于的Topic(字母顺序)：

Computational Social Science and Cultural Analytics - 计算社会科学与文化分析

Dialogue and Interactive Systems - 对话与互动系统

Discourse and Pragmatics - 话语分析与语用学

Ethics and NLP - 伦理与自然语言处理

Generation - 生成

Information Extraction - 信息提取

Information Retrieval and Text Mining - 信息检索与文本挖掘

Interpretability and Analysis of Models for NLP - NLP模型的可解释性与分析

Language Grounding to Vision, Robotics and Beyond - 语言视觉定位、机器人技术及其延伸

Large Language Models - 大型语言模型

Linguistic Diversity - 语言多样性

Linguistic Theories, Cognitive Modeling, and Psycholinguistics - 语言理论、认知建模与心理语言学

Machine Learning for NLP - 面向自然语言处理的机器学习

Machine Translation - 机器翻译

Multilingualism and Cross-Lingual NLP - 多语言和跨语言自然语言处理

NLP Applications - 自然语言处理应用

Phonology, Morphology, and Word Segmentation - 语音学、形态学与词汇分割

Question Answering - 问答系统

Resources and Evaluation - 资源和评估

Semantics: Lexical - 语义学：词汇

Semantics: Sentence-level Semantics, Textual Inference, and Other Areas - 语义学：句子水平语义学、文本推理及其他领域

Sentiment Analysis, Stylistic Analysis, and Argument Mining - 情感分析、文体分析与论点挖掘

Speech and Multimodality - 语音和多模态

Summarization - 总结

Syntax: Tagging, Chunking and Parsing - 句法：标注、分块与解析

Theme Track (see below) - 主题轨道（见下文）

## Accepted Rate

# LLM

## 大模型道德

- Knowledge of cultural moral norms in large language models
  url：https://aclanthology.org/2023.acl-long.26/
  code：https://github.com/AidaRamezani/cultural_inference
- Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models
  url：https://aclanthology.org/2023.acl-long.84/
  code：https://github.com/myracheng/markedpersonas

## 长文本推理

- Open-ended Long Text Generation via Masked Language Modeling
  url：https://aclanthology.org/2023.acl-long.13/
  code：https://github.com/dropreg/OpenLTG-MLM
- Efficient Streaming Language Models with Attention Sinks
  url：https://aclanthology.org/2023.acl-long.13/
  code：https://github.com/mit-han-lab/streaming-llm

## 大模型推理

- In-Context Analogical Reasoning with Pre-Trained Language Models
  url：https://aclanthology.org/2023.acl-long.109/
  code：https://github.com/hxiaoyang/lm-raven
- Semantic-Oriented Unlabeled Priming for Large-Scale Language Models
  url：https://aclanthology.org/2023.sustainlp-1.2/
- Decoding Symbolism in Language Models

  url：[https://aclanthology.org/2023.acl-long.186/](https://aclanthology.org/2023.acl-long.186/)

  code：[https://github.com/MeiqiGuo/ACL2023-SymbA](https://github.com/MeiqiGuo/ACL2023-SymbA)

## 高效解码（个人向）

- [**FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference**](https://aclanthology.org/2023.findings-acl.732/)
- [**Modular Transformers: Compressing Transformers into Modularized Layers for Flexible Efficient Inference**](https://aclanthology.org/2023.findings-acl.664/)
- [**BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting**](https://aclanthology.org/2023.acl-industry.48/)
- [**NarrowBERT: Accelerating Masked Language Model Pretraining and Inference](https://aclanthology.org/2023.acl-short.146/)： 着重在训练与推理时的吞吐**
- [**Accelerating Transformer Inference for Translation via Parallel Decoding**](https://aclanthology.org/2023.acl-long.689/)

## 大模型测评

- A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models

url：https://aclanthology.org/2023.acl-long.32/
      code：https://github.com/alestolfo/causal-math

- Do language models have coherent mental models of everyday things?
  url：https://aclanthology.org/2023.acl-long.106/
  code：https://github.com/allenai/everyday-things/
- We Understand Elliptical Sentences, and Language Models should Too: A New Dataset for Studying Ellipsis and its Interaction with Thematic Fit
  url：https://aclanthology.org/2023.acl-long.188/
  code：https://github.com/Caput97/ELLie-ellipsis_and_thematic_fit_with_LMs
- Can ChatGPT Understand Causal Language in Science Claims?
  url：https://aclanthology.org/2023.wassa-1.33/
- ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models
  url：https://aclanthology.org/2023.wassa-1.29/
  code：https://github.com/DLR-SC/JokeGPT-WASSA23
- Examining the Causal Impact of First Names on Language Models: The Case of Social Commonsense Reasoning
  url：https://aclanthology.org/2023.trustnlp-1.7/
  code：https://github.com/sullamij/Causal-First-Names/

## 数据生成

- Self-Instruct: Aligning LM with Self Generated Instructions
  url：https://aclanthology.org/2023.acl-long.754/
  code：https://github.com/yizhongw/self-instruct
- Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions
  url：https://aclanthology.org/2023.acl-long.34/
- Instruction Induction: From Few Examples to Natural Language Task Descriptions
  url：https://aclanthology.org/2023.acl-long.108/
  code：https://github.com/orhonovich/instruction-induction
- How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?
  url：https://aclanthology.org/2023.sustainlp-1.13/
  code：https://github.com/zjunlp/DeepKE/tree/main/example/llm

## 大模型微调

- Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models
  url：https://aclanthology.org/2023.acl-long.95/
- ADEPT: Adapter-based Efficient Prompt Tuning Approach for Language Models
  url：https://aclanthology.org/2023.sustainlp-1.8/
  code：https://github.com/Aditya-shahh/ADEPT

## 大模型与安全

- GPTs Don’t Keep Secrets: Searching for Backdoor Watermark Triggers in Autoregressive Language Models
  url：https://aclanthology.org/2023.trustnlp-1.21/
  code：https://github.com/evan-person/findingBackdoorWatermarks

## 受控生成

- [**NEUROSTRUCTURAL DECODING: Neural Text Generation with Structural Constraints**](https://aclanthology.org/2023.acl-long.528/)
- [**Critic-Guided Decoding for Controlled Text Generation**](https://aclanthology.org/2023.findings-acl.281/)
- [**The Whole Truth and Nothing But the Truth: Faithful and Controllable Dialogue Response Generation with Dataflow Transduction and Constrained Decoding**](https://aclanthology.org/2023.findings-acl.351/)
- [**PREADD: Prefix-Adaptive Decoding for Controlled Text Generation**](https://aclanthology.org/2023.findings-acl.636/)
- [**BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases**](https://aclanthology.org/2023.acl-short.18/)

## 解码质量增强

- [**The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers**](https://aclanthology.org/2023.acl-long.580/)
- [**Contrastive Decoding: Open-ended Text Generation as Optimization**](https://aclanthology.org/2023.acl-long.687/)
- [**RFiD: Towards Rational Fusion-in-Decoder for Open-Domain Question Answering**](https://aclanthology.org/2023.findings-acl.155/)
- [**Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding**](https://aclanthology.org/2023.findings-acl.262/)

## 显存节约

- [**Petals: Collaborative Inference and Fine-tuning of Large Models**](https://aclanthology.org/2023.acl-demo.54/)
