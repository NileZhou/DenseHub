[TOC]



# Introduction



https://chat.langchain.com/: ask any questions about langchain, sources from [chat-langchain](https://github.com/langchain-ai/chat-langchain)

https://changelog.langchain.com/: progress of whole langchain 



## LangChain

- LangChain-core: base abstractions and LCEL
- LangChain: LLMs app dev framework
- LangChain-community: Third party integrations



## LangGraph

Build robust and stateful multi-actor applications with LLMs in a graph, Integrates smoothly with LangChain, but can be used without it.

Built on top of LCEL, designed for more complex agents

It includes, support for cycles, and prioritizes controllability.

Advantages:

1. support streaming and asynchronous
2. more flexible
3. built-in persistence





## LangServe

**Deprecated by LangGraph Cloud**

Deploy LangChain chains as REST APIs.





## LangSmith



https://smith.langchain.com/



responsibilities:

- Tracing LLM gen task
- Monitoring tracing metrics visually
- Datasets: supports datasets and manager
- Evaluation
- Manage and debug prompts

It need create api keys



# History



v0.0.0: 

- LLMChain(Prompt Template -> Model -> Output parser)

- Heterogeneous data pipeline: Document Loader -> Document Transform -> Embed -> Retrieve

v0.2.x: 

- LCEL, make memory/Tools/Plan/Action to  **same level** , downgrade the abstraction level

Big changes:

- 0.2 removed something in 0.1: https://python.langchain.com/v0.1/docs/changelog/langchain/

- integration-agnostic (集成无关) principle:

  LangChain will **not instantiate** specific chat models、 LLM、embedding models or vector store

- Explicitly specified: user should specify 





# LCEL

features:

- **Unified interface**

  Every LCEL object implements the `Runnable` interface, which defines a common set of invocation methods (`invoke`, `batch`, `stream`, `ainvoke`, ...).

- **Composition primitives**

  LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internals, and more.

  

The **input type** and **output type** varies by component:

| Component    | Input Type                                            | Output Type           |
| ------------ | ----------------------------------------------------- | --------------------- |
| Prompt       | Dictionary                                            | PromptValue           |
| ChatModel    | Single string, list of chat messages or a PromptValue | ChatMessage           |
| LLM          | Single string, list of chat messages or a PromptValue | String                |
| OutputParser | The output of an LLM or ChatModel                     | Depends on the parser |
| Retriever    | Single string                                         | List of Documents     |
| Tool         | Single string or dictionary, depending on the tool    | Depends on the tool   |





## pipe operator



eg:

```python
chain = prompt | model | output_parser
```



## Runnable Interface



Components implemented runnable, then the chain consists of components is also implemented runnable

methods of runnable interface:

- `stream`: stream back chunks of the response

- `invoke`: call the chain on an input

  ```python
  ```

  

- `batch`: call the chain on a list of inputs



all runnable objects should explicitly illustrates their input/output schema, to check the I/O.

- input schema: Pydantic model generated by Runnable object
- output schema: Pydantic model generated by Runnable object