# Manipulate the digital world

close source app:

* [https://www.mobile.dev/app-quality-copilot](https://www.mobile.dev/app-quality-copilot)
* doc: [https://maestro.mobile.dev/api-reference/commands](https://maestro.mobile.dev/api-reference/commands)

UI测试自动化:

站在人类视角，会如何手动完成目标中的测试用例呢？

1. **眼睛** ：理解页面关键 UI 元素（搜索框、搜索按钮、搜索结果）
2. **手** ：根据测试用例执行执行对应操作
3. **大脑** ：协调控制眼睛和手，把整个流程串起来

同理，站在 LLM 视角，咱们也可以基于上述关键步骤依次还原，但这里涉及到一些工具和方案选型，大概有以下选择：

1. **眼睛** ：GPT4V、通义千问、Kimi 等
2. **手** ：[Art](https://bytedance.larkoffice.com/wiki/GW1JwFluKiHvBhk00tlc0msonmf)、Maestro 等
3. **大脑** ：autogen、coze 等

理论上我们基于以上工具和方案就能实现目标，我将会在 “实践” 部分分析和实践具体的技术选型

open source projects:

* screenshot2Code

从截图到代码

[https://github.com/abi/screenshot-to-code](https://github.com/abi/screenshot-to-code)


paper list:

[https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List)
