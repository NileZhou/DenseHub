PS: Infereence server is not model inference framework, but rather a server

# Triton

https://www.cnblogs.com/zackstang/p/18269743

https://github.com/triton-inference-server/server

notice: not confused with (https://github.com/triton-lang/triton, supported by OpenAI, this is a deeplearning language+compiler, equal level with TVM, Taichi)
