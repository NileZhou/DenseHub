PS: Infereence server is not model inference framework, but rather a server

# Inference Server

## Triton

https://www.cnblogs.com/zackstang/p/18269743

https://github.com/triton-inference-server/server

notice: not confused with (https://github.com/triton-lang/triton, supported by OpenAI, this is a deeplearning language+compiler, equal level with TVM, Taichi)







# API Server



## litellm


Call all LLM APIs using the OpenAI format. Use Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate, Groq (100+ LLMs)


[litellm](https://github.com/BerriAI/litellm)


support:

- Async
- Streaming
- Logging Observability
- OpenAI Proxy
