
- 2024.10:  

2000 张 GPU 训练一个半月，只花了 300 多万美金，做出来的预训练模型跟 Grok 打平，只花它的 1% 或 2% 的成本
这次混合注意力机制是我们做的一个非常重要的点，混合注意力是计算里面比较大的比例，我们不但把KV cache缩小很多，将部分层的计算复杂度从 L 平方降到了 L。   

再下面是一个MoE，也就是混合专家的模型，混合专家里面有很多的专家，我们专家相当多，但有一点是我们的新发明，专家很多，但不一定每次都要用那么多。我们训练时假设有80个专家，每次都调用12个或15个，在推理的时候是否可以少调几个，这样可以省掉很多的时间，这些都可以用动态的方法。    

通俗点说，对简单的问题问两个专家，难的问题问十个专家，就跟人们所碰到的问题一样，当问题简单，想知道天气是什么的时候，不用找一堆专家来浪费他们的时间，但问题很复杂的时候，或许真的需要很多的专家，这样的平衡能不太影响我们的表现，但能节省很多时间。   

最后一点非常重要，因为能做多阶段的训练，我们可以把整个训练切成两块，有一块是做好以后就把它固定起来了，在这个固定的模型上面再做后端的训练，所以我们不必重复一个又一个的实验，我们研究员有五个方法，他们不必把全部的数据跑五遍才看哪个最好，我们可以80%或90%都是固定起来，最后再去做高效的对比。   

这个主要的对我们的好处就是我们可以用最低的成本多训练一些不同的模型，从中学习到怎样去用不同的算法，哪些算法表现最好，所以多阶段训练也是独特的和前所未有的。   



- 2023.11:

黄文灏​
Always Day One
亲自答
此回答由问题相关方亲自撰写
王焱 等 1312 人赞同了该回答  
很荣幸参与了模型的训练过程，整个过程中最大的感触就是scale up is all you need，大模型是极致的系统科学，基础做好了之后模型在scale up的过程中会无比“丝滑”。34B只是一个很小的节点，千亿和万亿模型仍然可以按照这套系统科学顺利产出结果。相信训练过程中的很多point和tricks大家也都耳熟能详了，就不一一展开了，记录几个比较有趣的insight吧，希望能产生一些价值。引用老王的一句话，在追逐AGI的道路上大家都是同行者。  

要有自己的评测指标  
通用的benchmark特别像MMLU这种比较综合的指标能反映模型的一部分能力，但拿这些benchmark作为优化目标，模型训练过程中非常容易走偏或者在in-domain data上过度优化。我们花了很长的时间探索哪些指标能真正反映模型的能力，哪些指标是和智能强相关的。举个例子，最近很多人在讨论压缩和智能的关系，其实压缩率就是个很好的评价指标。但用压缩率作为指标时会发现它是和数据强相关的，在不同的数据分布上压缩率不可比，怎么去建立数据分布和压缩率的关系就很值得去深入研究一下。  

当我们建立了自己的评测指标后，按照评测指标去优化模型的训练过程，团队内部就会对模型有很强的自信，知道做的所有工作都是在优化模型的能力。最后再去benchmark上评测训练完的模型，会发现模型指标还是很不错的，这个算是个副产物吧，情理之中也意料之内。  

数据越多越好，质量越高越好  
这好像已经是老生常谈的问题了，不过还是要拎出来再强调一下。为了能有最丰富的数据源和原始数据，我们处理了所有cc的数据，比单纯处理几个cc的数据包还是会多出大量的数据。但我们也用了更加严格的数据筛选标准，和falcon，ccnet这些开源的数据处理方案相比，我们最后保留的数据量不到其它方法的1/10。这种严格的数据筛选也是后面模型有良好表现的基础。  

当处理完所有的cc数据以后，一个insight是所有做大模型预训练的公司都会有能力且必须把互联网上所有的数据处理一遍，好像大家都可以做一遍搜索引擎了。另一方面，很多人在讨论文本世界不够了的问题，其实可以想想被筛掉的数据道理有没有信息量。除了重复的那部分数据以外，其它数据理论上都是有压缩的价值的。从这个维度上看，能使用的文本数据比我们现在使用的几十T token还能有几个数量级的提升。  

Scaling Law is all you need
很多人都认为scaling law就是用来算最优的数据和参数量的一个公式，但其实scaling law能做的事情远不止如此。为了真正理解scaling law，要做的第一件事就是忘记Chinchilla Scaling Law，然后打开OpenAI的Scaling Law的paper，再把paper中OpenAI引用自己的更早的paper都详细的读几十遍（实在是有点晦涩难懂），有条件的把里面的实验都复现一下（其实需要的实验资源不多），然后就会感到如同发现了新大陆一般。这里要感谢 @Zhi Tian 和 @曹越 在这个方向上的讨论和指导，国内的大神在核心问题的理解上绝对是世界最顶尖的水平的。

理解了Scaling law，就会发现大模型的training dynamics是完全可以建模的，training performance是完全可预测的。在100M以下的模型上做的实验拟合的公式，可以准确预测几十B甚至上百B模型训练时每一步的validation loss（实际上几百步预测一次就够了）。有了这个基础，所有的实验就只要在小模型上做就可以了。我们在几十B和几百B模型上的实验都是一次完成的。

预训练模型的平衡性很重要
预训练模型作为基座其实并没有太多直接做应用的能力，需要后面大量post training或者alignment的工作。我们的一个结论是在预训练阶段IQ和EQ要尽可能平衡，“理科生”能力和“文科生”能力不能有严重偏科。其实现在在productivity方向（如ChatGPT，Copilot）和entertainment方向（如Character.AI和Inflection PI）需要的分别是模型的IQ和EQ能力。但这两个能力在预训练阶段会互相影响，因此控制两者的平衡很关键。可以看到大量针对代码和数学优化的模型很难用在娱乐场景上，从一定程度上限制了基座模型的应用范围。同时，我们的研究发现在提升模型代码和数学能力的过程中，只要做到合理的数据配比，并不会影响模型在其它方面的性能。所以我们在pre training阶段用了尽可能平衡的数据比例，把专项能力交给post training阶段。在内部不同版本的post training模型上，我们发现这种方法对未来模型的可提升性有比较大的帮助，同时也能让IQ和EQ两部分数据在预训练阶段互相促进，使预训练模型的潜能更大。

人均GPU数 >> 人数
最后要shout out to预训练团队，能沉下心来好好处理数据，研究training dynamics，扎扎实实地把所有基础都打好。顺利的scale up的过程也证明了所有基础工作的价值。也谢谢开复老师给每个人充分的信任，团队人数不多，但每个人都有大量的GPU资源。也进一步证明了大模型训练人数并不关键（很多时候会是反向指标），人均GPU数才是最关键的指标。

我们相信，模型参数还可以有几个数量级的scale up空间，训练数据还有几个数量级的scale up空间，大模型的智能也会有很大的scale up空间。现在才刚刚起步，scale up is all you need！


core person: [Wenhao Huang](https://scholar.google.com/citations?hl=zh-CN&user=OdE3MsQAAAAJ&view_op=list_works&sortby=pubdate)
