LLMSurvey: [LLMSurvey](awesome-distributed-systems)

Key-ML-Models: [https://github.com/dair-ai/ML-Papers-Explained](https://github.com/dair-ai/ML-Papers-Explained)





[TOC]



# Model Architecture



## Dense or MoE:

### Dense

- Transformer: Decoder-Only
- Mamba-2
- Jamba
- TTT
- RWKV

### MoE


- Mistral

- Deepseek

  https://huggingface.co/deepseek-ai/DeepSeek-V2.5



## Multimodal



### Vision



- Qwen2-VL

  https://github.com/QwenLM/Qwen2-VL

- Pixtral 12B

  https://mistral.ai/news/pixtral-12b

- InternVL

  https://github.com/OpenGVLab/InternVL



### Voice

ASR & TTS

- GLM-4-Voice

  https://github.com/THUDM/GLM-4-Voice

  



### Unified



- Emu3

  https://github.com/baaivision/Emu3

  paper: [Emu3: Next-Token Prediction is All You Need](https://arxiv.org/abs/2409.18869)





## Model as Agent

- Qwen-Agent

  https://github.com/QwenLM/Qwen-Agent






# Chat Model

# Vision QA Model

## Image QA Model

## Video QA Model

# Code Model

Note: llama3 is better than codellama-70B (based on llama2-70B, then continual pretrain on code dataset), [see](https://mistral.ai/news/codestral/)

# Function Model

# Cost

## Closed-source Models

## Open-source Models
